<?xml version="1.0" encoding="utf-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!doctype html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="de-de" lang="de-de" dir="ltr" >
<!-- !head -->
   <head>
	   
	   
	   	<style>
	.navigation {
    text-align: right;
    background-color: #333;
		
    padding: 10px 20px;
  }

  .navigation ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .navigation li {
    display: inline-block;
    margin-right: 20px;
  }

  .navigation a {
    text-decoration: none;
    color: #fff;
    font-weight: bold;
  }

  .navigation a:hover {
    text-decoration: underline;
  }
			
			  body {
    margin: 0;
    padding: 0;
    font-family: Arial, sans-serif;
				 
    background-color: #f0f5f9;
	      background-image: url('background.jpeg'); /* Add your background image URL here */
    background-size: cover;
    background-repeat: no-repeat;
    background-attachment: fixed;
  }
		</style>
	   
	   
      <meta http-equiv="content-type" content="text/html; charset=utf-8" />
      <meta name="robots" content="noindex,nofollow" />
      <meta name="keywords" content="Publications" />
      <meta name="description" content="Our Publications " />

      <title>Publications  </title>

	   
      <link rel="stylesheet" media="screen" href="css/main.css" type="text/css" />


      <link rel="stylesheet" media="print" href="css/print.css" type="text/css" />

  

	
      <script src="lib/MooTools-Core-1.6.0.js" type="text/javascript"></script>
      <script type="text/javascript">
window.addEvent('domready', function() {
// inject text and test size; if font is loaded, it's applied to body
   var fTest = new Element('span#font',{html: 'mmmmmmmmmmmmmmmmmmmm'});
   $$(document.body).adopt(fTest);
   var dim = $('font').getSize();
   if (dim.x < 210) {$$(document.body).addClass('font')}
   $('font').destroy();

// add functionality to remove additional info from publications
   $$('.publications').each(function(myList,ind){
      var btn1 = new Element('span',{
         'class': 'publist',
         'html': 'show citations only',
         'events': {
            'click': function(){
               myList.toggleClass('raw');
               if (myList.className == 'publications raw') {
                  btn1.set('text','show more details');
               } else {
                  btn1.set('text','show citations only');
               };
            }
         }
      });
      btn1.inject(myList,'top');



   });


});
      </script>
	   
	   
	   

   </head>
<!-- head -->

   <body>
	   
	   <div class="navigation">
  <ul>
    <li style="font-size: 18px;  color: #FFFFFF" ><a href="index.html">HOME</a></li>
    <li style="font-size: 18px;  color: #FFFFFF"><a href="people.html">PEOPLE</a></li>
    <li style="font-size: 18px;  color: #FFFFFF"><a href="publications.html">PUBLICATIONS</a></li>
  </ul>
</div>
	
	   
<!-- !outer_wrapper -->
      <div id="outer_wrapper">


		  <header> <a href="">
    
    </a>

			  </header>


<div id="showcase" style="background-image: url(media/backgrounds/IMG_9809.jpg)">

</div>


<!-- !inner_wrapper -->
         <div id="inner_wrapper">

<!-- !subnavigation -->
            <ul id="subnavigation">
              <li>
                  <a href="#preprint">preprint</a>
              </li>
              <li>
                    <a href="#year23">2023</a>
              </li>
              <li>
                    <a href="#year22">2022</a>
              </li>
              <li>
                    <a href="#year21">2021</a>
              </li>
              <li>
                  <a href="#year20">2020</a>
               </li>
               <li>
                  <a href="#year19">2019</a>
               </li>
               <li>
                  <a href="#year18">2018</a>
               </li>
               <li>
                  <a href="#year17">2017</a>
               </li>
               <li>
                  <a href="#year16">2016</a>
               </li>
               <li>
                  <a href="#year15">2015</a>
               </li>

               <li>
                  <a href="#year13">2013</a>
               </li>
               <li>
                  <a href="#year12">2012</a>
               </li>

               <li>
                  <a href="#year10">2010</a>
               </li>

            </ul>
<!-- subnavigation -->

<!-- !content -->
            <div id="content">


<!-- !publications -->
<div class="publications">
  <a id="preprint" />
     <h1>
        preprint
     </h1>
     <ul>

     <!-- pub -->



<!-- pub -->


         <!-- pub -->
         <li>
              <h2>
                 <a href="https://arxiv.org/pdf/2009.14487" target="_blank">
                    The Utility of Decorrelating Colour Spaces in Vector Quantised Variational Autoencoders
                 </a>
              </h2>
              <div class="pic">
                <img src="images/ColorConversionVAE.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="135"/>
              </div>
              <dl>
                 <dt class="authors">Authors:</dt><dd>Akbarinia, Gil-Rodríguez, Flachot & Toscani</dd>
                 <dt class="journal">Journal:</dt><dd>bioRxiv</dd>
                 <dt class="summary">Summary:</dt><dd class="summary">
While images are often represented in RGB colour space, the specific organisation of colours in other spaces also offer interesting features, e.g. CIE L*a*b* decorrelates chromaticity into opponent axes. We propose colour space conversion, a simple quasi-unsupervised task, to enforce a network learning structured representations.
                 <dt class="citation">Citation:</dt><dd class="citation"><cite>
                 Akbarinia, A., Gil-Rodríguez, R., Flachot, A., & Toscani, M. (2020). The Utility of Decorrelating Colour Spaces in Vector Quantised Variational Autoencoders. arXiv preprint arXiv:2009.14487.
                 </cite></dd>


              </dl>
              <br class="clear" />
           </li>
           <!-- !pub -->
         </ul>

         <a id="year23" />
            <h1>
               2023
            </h1>
            <ul>
              <li>
                   <h2>
                      <a href= "https://opg.optica.org/josaa/fulltext.cfm?uri=josaa-40-3-A190" target="_blank">
                         Perception of saturation in natural objects
                      </a>
                   </h2>
                   <div class="pic">
                     <img src="images/saturationSlope.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
                   </div>
                   <dl>
                      <dt class="authors">Authors:</dt><dd>Hedjar, Toscani & Gegenfurtner</dd>
                      <dt class="journal">Journal:</dt><dd>JOSA</dd>
                      <dt class="summary">Summary:</dt><dd class="summary">
                        The distribution of colors across a surface depends on the interaction between its surface properties, its shape, and the lighting environment. Shading, chroma, and lightness are positively correlated: points on the object that have highluminancealsohavehighchroma.Saturation,typically defined as the ratio of chroma to lightness,is therefore relatively constant across an object. Here we explored to what extent this relationship affects perceived saturation ofanobject. Using images of hyperspectral fruit and rendered matte objects, we manipulated the lightness–chroma correlation (positive or negative) and asked observers which of two objects appeared more saturated. Despite the negative-correlation stimulus having greater mean and maximum chroma, lightness, and saturation than the positive, observers overwhelmingly chose the positive as more saturated. This suggests that simple colorimetric statistics do not accurately represent perceived saturation of objects—observers likely base their judgments on interpretations about the cause of the color distribution.

                         <dt class="citation">Citation:</dt><dd class="citation"><cite>
                      Hedjar, L., Toscani, M., & Gegenfurtner, K. R. (2023). Perception of saturation in natural objects. JOSA A, 40(3), A190-A198.
                    </cite></dd>
                         </dl>
                   <br class="clear" />
                </li>

  <a id="year22" />
     <h1>
        2022
     </h1>
     <ul>
       <!-- pub -->
       <li>
            <h2>
               <a href= "https://elifesciences.org/articles/64876" target="_blank">
                  Unsupervised learning of haptic material properties
               </a>
            </h2>
            <div class="pic">
              <img src="images/UnsupervisedPreprint.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
            </div>
            <dl>
               <dt class="authors">Authors:</dt><dd>Metger & Toscani</dd>
               <dt class="journal">Journal:</dt><dd>eLife</dd>
               <dt class="summary">Summary:</dt><dd class="summary">
                 When touching the surface of an object, its spatial structure translates into a vibration on the skin. Here we show that perceptual haptic representation of materials emerges from efficient encoding of vibratory patterns elicited by the interaction with materials.

                  <dt class="citation">Citation:</dt><dd class="citation"><cite>
               Metzger A., & Toscani, M. (2022). Unsupervised learning of haptic material properties. eLife, 11, e64876.
             </cite></dd>
                  </dl>
            <br class="clear" />
         </li>

         <li>
              <h2>
                 <a href= "https://link.springer.com/chapter/10.1007/978-3-031-06249-0_36" target="_blank">
                    A Database of Vibratory Signals from Free Haptic Exploration of Natural Material Textures and Perceptual Judgments (ViPer): Analysis of Spectral Statistics
                 </a>
              </h2>
              <div class="pic">
                <img src="images/ViPerMaterials.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
              </div>
              <dl>
                 <dt class="authors">Authors:</dt><dd>Metger & Toscani</dd>
                 <dt class="journal">Journal:</dt><dd>International Conference on Human Haptic Sensing and Touch Enabled Computer Applications</dd>
                 <dt class="summary">Summary:</dt><dd class="summary">
                   We recorded vibratory patterns elicited by free haptic exploration of a large set of natural textures. Participants also provided judgments about the samples they explored. Vibratory signals can be approximated by a single parameter in the temporal frequency domain, in a similar way as we can describe the spatial frequency spectrum of natural images.

                    <dt class="citation">Citation:</dt><dd class="citation"><cite>
                 Metzger A., & Toscani, M. (2022). A Database of Vibratory Signals from Free Haptic Exploration of Natural Material Textures and Perceptual Judgments (ViPer): Analysis of Spectral Statistics. International Conference on Human Haptic Sensing and Touch Enabled Computer Applications, (pp. 319-327). Springer, Cham.
               </cite></dd>
                    </dl>
              <br class="clear" />
           </li>

	     <li>
            <h2>
               <a href= "https://link.springer.com/article/10.1007/s42979-021-00855-7" target="_blank">
                  Colour Calibration of a Head Mounted Display for Colour Vision Research Using Virtual Reality
               </a>
            </h2>
            <div class="pic">
              <img src="images/Screenshot 2022-03-06 at 21.53.06.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
            </div>
            <dl>
               <dt class="authors">Authors:</dt><dd>Raquel Gil Rodríguez, Florian Bayer, Matteo Toscani, Dar’ya Guarnera, Giuseppe Claudio Guarnera, & Karl R Gegenfurtner</dd>
               <dt class="journal">Journal:</dt><dd>SN Computer Science</dd>
               <dt class="summary">Summary:</dt><dd class="summary">
                Virtual reality (VR) technology offers vision researchers the opportunity to conduct immersive studies in simulated real-world scenes. Here, we propose a framework for calibrating an HMD using an imaging colorimeter, the I29 (Radiant Vision Systems, Redmond, WA, USA). In addition, we present a colour constancy experiment design for VR through the Unreal Engine 4.
                  <dt class="citation">Citation:</dt><dd class="citation"><cite>
               Gil Rodríguez, R., Bayer, F., Toscani, M. et al. Colour Calibration of a Head Mounted Display for Colour Vision Research Using Virtual Reality. SN COMPUT. SCI. 3, 22 (2022).
             </cite></dd>
                  </dl>
            <br class="clear" />
         </li>

         <!-- pub -->
        <!-- !pub -->


          <!-- !pub -->

       <!-- pub -->

  <a id="year21" />
     <h1>
        2021
     </h1>
     <ul>
       <!-- pub -->
       <li>
            <h2>
               <a href="https://jov.arvojournals.org/article.aspx?articleid=2772684" target="_blank">
                  Underconfidence in peripheral vision
               </a>
            </h2>
            <div class="pic">
              <img src="images/Confidence.gif" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
            </div>
            <dl>
               <dt class="authors">Authors:</dt><dd>Toscani, Mamassian & Valsecchi</dd>
               <dt class="journal">Journal:</dt><dd>Journal of Vision</dd>
               <dt class="summary">Summary:</dt><dd class="summary">
                Our visual experience appears uniform across the visual field, despite the poor resolution of peripheral vision. We show that humans are underconfident in peripheral judgements, thus metacognitive biases cannot explain our impression of uniformity, as this would require peripheral overconfidence.
                  <dt class="citation">Citation:</dt><dd class="citation"><cite>
               Toscani, M., Mamassian, P., & Valsecchi, M. (2021). Underconfidence in peripheral vision. Journal of Vision, 21(6), 2-2.
             </cite></dd>
                  </dl>
            <br class="clear" />
         </li>

         <!-- pub -->
        <!-- !pub -->

<li>
     <h2>
        <a href="papers/MetzgerToscaniValsecchiDrewing2021.pdf" target="_blank">
           Target search and inspection strategies in haptic search
        </a>
     </h2>
     <div class="pic">
<img src="images/metzgertoscanivalsecchidrewing.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
     </div>
     <dl>
        <dt class="authors">Authors:</dt><dd>Metzger, Toscani, Valsecchi & Drewing</dd>
        <dt class="journal">Journal:</dt><dd>IEEE TRANSACTIONS ON HAPTICS</dd>
        <dt class="summary">Summary:</dt><dd class="summary">
          In a search task, after a potential target is detected by any of the fingers, there is higher probability that subsequent exploration is performed by the index or the middle finger. At the same time, these fingers dramatically slow down. This suggests that the middle and the index finger are specialized for fine analysis in haptic search.
           <dt class="citation">Citation:</dt><dd class="citation"><cite>
             Metzger, A., Toscani, M., Valsecchi, M., & Drewing, K. (2021). Target search and inspection strategies in haptic search. IEEE Transactions on Haptics.
           </cite></dd>
           </dl>
     <br class="clear" />
  </li>

  <!-- pub -->
  <!-- !pub -->

<li>
     <h2>
        <a href="https://jov.arvojournals.org/article.aspx?articleid=2772274" target="_blank">
           Surface properties and the perception of color
        </a>
     </h2>
     <div class="pic">
       <img src="images/GlossColorKim.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="110" height="100"/>
     </div>
     <dl>
        <dt class="authors">Authors:</dt><dd>Isherwood, Huynh-Thu, Arnison, Monaghan, Toscani, Perry, Honson, & Kim</dd>
        <dt class="journal">Journal:</dt><dd>Journal of Vision</dd>
        <dt class="summary">Summary:</dt><dd class="summary">
          We examined whether perception of color saturation and lightness depends on the three-dimensional (3D) shape and surface gloss. Findings suggest that perceived color saturation and lightness depend on the separation of specular highlights from diffuse shading informative of chromatic surface reflectance.
        <dt class="citation">Citation:</dt><dd class="citation"><cite>
        Isherwood, Z. J., Huynh-Thu, Q., Arnison, M., Monaghan, D., Toscani, M., Perry, S., ... & Kim, J. (2021). Surface properties and the perception of color. Journal of vision, 21(2), 7-7.
        </cite></dd>
     </dl>
     <br class="clear" />
  </li>
  <!-- pub -->
  <!-- !pub -->

       <li>
            <h2>
               <a href="https://www.nature.com/articles/s41598-020-80675-6" target="_blank">
                  Deep neural network model of haptic saliency
               </a>
            </h2>
            <div class="pic">
              <img src="images/TouchSaliencyCrop.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
            </div>
            <dl>
              <dt class="authors">Authors:</dt><dd>Metzger, Toscani, Akbarinia, Valsecchi, Drewing</dd>
               <dt class="journal">Journal:</dt><dd>Scientific Reports</dd>
               <dt class="summary">Summary:</dt><dd class="summary">
                 Haptic exploration usually involves stereotypical systematic movements that are adapted to the task. Here we show that haptic exploratory behavior is to some extent driven by the physical features of the stimuli, with e.g. edge-like structures, vertical and horizontal patterns, and rough regions being explored in more detail.


                  <dt class="citation">Citation:</dt><dd class="citation"><cite>
               Metzger, A., Toscani, M., Akbarinia, A., Valsecchi, M., & Drewing, K. (2021). Deep neural network model of haptic saliency. Scientific reports, 11(1), 1-14.
             </cite></dd>

            </dl>
            <br class="clear" />
         </li>

          <!-- !pub -->

       <!-- pub -->


  <a id="year20" />
     <h1>
        2020
     </h1>
     <ul>
       <!-- pub -->
              <li>
                   <h2>
                      <a href="papers/tap_paper_final.pdf" target="_blank">
                         Three perceptual dimensions for specular and diffuse reflection
                      </a>
                   </h2>
                   <div class="pic">
                     <img src="images/Metallicity.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
                   </div>
                   <dl>
                      <dt class="authors">Authors:</dt><dd> Toscani, Guarnera, Guarnera, Hardeberg, Gegenfurtner</dd>
                      <dt class="journal">Journal:</dt><dd>ACM Transactions on Applied Perception (TAP)</dd>
                      <dt class="summary">Summary:</dt><dd class="summary">
                        Previous research investigated the perceptual dimensionality of achromatic reflection of opaque surfaces, by using either simple analytic models of reflection, or measured reflection properties of a limited sample of materials. Here we extend this work to a broader range of simulated materials.
                      <dt class="citation">Citation:</dt><dd class="citation"><cite>
                        Toscani, M., Guarnera, D. Y., Guarnera, G. C., Hardeberg, J. Y., & Gegenfurtner, K. R. (2020). Three perceptual dimensions for specular and diffuse reflection. ACM Transactions on Applied Perception (TAP), 17(2), 1-26.
                      </cite></dd>

                   </dl>
                   <br class="clear" />
                </li>
                <!-- pub -->
               <!-- !pub -->

              <li>
                   <h2>
                      <a href="https://jov.arvojournals.org/article.aspx?articleid=2765045" target="_blank">
                         Color consistency in the appearance of bleached fabrics
                      </a>
                   </h2>
                   <div class="pic">
                       <img src="images/Bleach.png" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
                   </div>
                   <dl>
                      <dt class="authors">Authors:</dt><dd>Toscani, Milojevic, Fleming, Gegenfurtner</dd>
                      <dt class="journal">Journal:</dt><dd>Journal of Vision</dd>
                      <dt class="summary">Summary:</dt><dd class="summary">
                         Human observers are remarkably good at perceiving constant object color across illumination changes. However, there are numerous other factors that can modulate surface appearance, such as aging, bleaching, staining, or soaking. Here we investigated whether humans could reproduce the original color of bleached fabrics.
                      <dt class="citation">Citation:</dt><dd class="citation"><cite>
                      Toscani, M., Milojevic, Z., Fleming, R. W., & Gegenfurtner, K. R. (2020). Color consistency in the appearance of bleached fabrics. Journal of vision, 20(4), 11-11.
                      </cite></dd>

                   </dl>
                   <br class="clear" />
                </li>

                <!-- pub -->
               <!-- !pub -->

              <li>
                   <h2>
                      <a href="papers/newdresses_manu_R_clean.pdf" target="_blank">
                         How to make a #theDress
                      </a>
                   </h2>
                   <div class="pic">
                       <img src="images/getThumbnail.jpeg" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>
                   </div>
                   <dl>
                      <dt class="authors">Authors:</dt><dd>Witzel, Toscani </dd>
                      <dt class="journal">Journal:</dt><dd>JOSA A</dd>
                      <dt class="summary">Summary:</dt><dd class="summary">
                         If we completely understand how a phenomenon works, we should be able to produce it ourselves. Here, we developed a simple algorithm that transforms any image with bicolored objects into an image with the properties of #theDress.
                      <dt class="citation">Citation:</dt><dd class="citation"><cite>
                      Witzel, C., & Toscani, M. (2020). How to make a# theDress. JOSA A, 37(4), A202-A211.
                      </cite></dd>

                   </dl>
                   <br class="clear" />
                </li>

                <!-- pub -->
               <!-- !pub -->


       <!-- pub -->

       <!-- !pub -->
    <a id="year19" />
       <h1>
          2019
       </h1>
       <ul>

  <!-- pub -->
  <li>
       <h2>
          <a href="https://journals.sagepub.com/doi/full/10.1177/2041669519889070" target="_blank">
             Gloss and speed judgments yield different fine tuning of saccadic sampling in dynamic scenes
          </a>
       </h2>
       <div class="pic">
         <img src="images/GlossMotion2.gif" alt="scene 2,3,7,8" title="scene 2,3,7,8" width="100" height="100"/>

       </div>
       <dl>
          <dt class="authors">Authors:</dt><dd>Toscani, Yücel, Doerschner</dd>
          <dt class="journal">Journal:</dt><dd>i-Perception</dd>
          <dt class="summary">Summary:</dt><dd class="summary">
             An introduction to unsupervised deep learning approaches to vision, which advocates for such 'statistical appearance models' as being more appropriate ways to think about material perception than classic 'inverse optics' frameworks.
           </dd>
          <dt class="citation">Citation:</dt><dd class="citation"><cite>
        Toscani, M., Yücel, E. I., & Doerschner, K. (2019). Gloss and speed judgments yield different fine tuning of saccadic sampling in dynamic scenes. i-Perception, 10(6), 2041669519889070.
          </cite></dd>

       </dl>
       <br class="clear" />
    </li>

    <!-- pub -->

    <!-- !pub -->

    <li>
         <h2>
            <a href="https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2675949/SITIS_2019_paper.pdf?sequence=1" target="_blank">
               Assessment of OLED Head Mounted Display for Vision Research with Virtual Reality
            </a>
         </h2>
         <div class="pic">
<img src="images/VRgammas.png" alt="data plot" title="data plot" width="100" height="100"/>
         </div>
         <dl>
            <dt class="authors">Authors:</dt><dd>Toscani, Gil, Guarnera, Guarnera, Kalouaz, Gegenfurtner</dd>
            <dt class="journal">Journal:</dt><dd>SITIS</dd>
            <dt class="summary">Summary:</dt><dd class="summary">
             We show that a common VR display used in our experiments behaves similarly to a standard OLED monitor. The VR gaming engine we tested (Unreal Engine 4) introduces a complex behavior, which can be disabled. This allows to accurately control colors and luminance emitted by the display.
             </dd>
            <dt class="citation">Citation:</dt><dd class="citation"><cite>
            Toscani, M., Gil, R., Guarnera, D. Y., Guarnera, G., Kalouaz, A., & Gegenfurtner, K. R. (2019, November). Assessment of OLED Head Mounted Display for Vision Research with Virtual Reality. In 2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS) (pp. 738-745). IEEE.
            </cite></dd>

         </dl>
         <br class="clear" />
      </li>

      <!-- pub -->


    <!-- !pub -->

    <li>
         <h2>
            <a href="https://journals.sagepub.com/doi/pdf/10.1177/2041669519884335" target="_blank">
               Lightness discrimination depends more on bright rather than shaded regions of three-dimensional objects
            </a>
         </h2>
         <div class="pic">
           <img src="images/LightnessDiscrimination.png" alt="icon" title="icon" width="100" height="100"/>
         </div>
         <dl>
            <dt class="authors">Authors:</dt><dd>Toscani, Valsecchi</dd>
            <dt class="journal">Journal:</dt><dd>i-Perception</dd>
            <dt class="summary">Summary:</dt><dd class="summary">
              The brighter portions of a shaded complex object are in principle more informative about its lightness and are preferentially fixated during lightness judgments. Our results show that the strategy of relying on the brightest areas of a complex object is functionally optimal, yielding more precise lightness judgements.
              </dd>
            <dt class="citation">Citation:</dt><dd class="citation"><cite>
            Toscani, M., & Valsecchi, M. (2019). Lightness discrimination depends more on bright rather than shaded regions of three-dimensional objects. i-Perception, 10(6), 2041669519884335.
            </cite></dd>

         </dl>
         <br class="clear" />
      </li>


      <!-- pub -->
      <!-- !pub -->

      <li>
           <h2>
              <a href="/papers/metzger_search_2019" target="_blank">
                 Dynamics of exploration in haptic search
              </a>
           </h2>
           <div class="pic">
               <img src="images/metzger_search_2019.png" alt="shape transformations" title="shape transformations" width="100" height="100"/>
           </div>
           <dl>
              <dt class="authors">Authors:</dt><dd>Metzger, Toscani, Valsecchi, Drewing</dd>
              <dt class="journal">Journal:</dt><dd>WHC</dd>
              <dt class="summary">Summary:</dt><dd class="summary">
                Haptic search is a common every day task. We show that haptic search consists of two phases: a process of target search using all fingers, and a target analysis using the middle and the index finger, which might be specialized for fine analysis.
              </dd>
              <dt class="citation">Citation:</dt><dd class="citation"><cite>
              Metzger, A., Toscani, M., Valsecchi, M., & Drewing, K. (2019, July). Dynamics of exploration in haptic search. In 2019 IEEE World Haptics Conference (WHC) (pp. 277-282). IEEE.
              </cite></dd>
              <dt class="data">Ressources</dt><dd><a href="https://zenodo.org/record/2247284" target="_blank">Download data &amp; Code</a></dd>

           </dl>
           <br class="clear" />
        </li>



  <a id="year18" />
     <h1>
        2018
     </h1>
     <ul>

  <!-- !pub -->

  <li>
       <h2>
          <a href="[https://eprints.whiterose.ac.uk/153581/1/perceptually_validated_accepted_TVCG.pdf" target="_blank">
             Perceptually validated cross-renderer analytical BRDF parameter remapping
          </a>
       </h2>
       <div class="pic">
         <img src="images/remapping.png" alt="shape transformations" title="shape transformations" width="100" height="100"/>
       </div>
       <dl>
          <dt class="authors">Authors:</dt><dd>Guarnera, Guarnera, Toscani, Glencross,Li, Hardeberg, Gegenfurtner</dd>
          <dt class="journal">Journal:</dt><dd>IEEE transactions on visualization and computer graphics</dd>
          <dt class="summary">Summary:</dt><dd class="summary">
            We present a novel BRDF remapping technique, that automatically computes a mapping (BRDF Difference Probe) to match the appearance of a source material model to a target one.
          <dt class="citation">Citation:</dt><dd class="citation"><cite>
        Guarnera, D. Y., Guarnera, G. C., Toscani, M., Glencross, M., Li, B., Hardeberg, J. Y., & Gegenfurtner, K. R. (2018). Perceptually validated cross-renderer analytical BRDF parameter remapping. IEEE transactions on visualization and computer graphics, 26(6), 2258-2272.
          </cite></dd>

       </dl>
       <br class="clear" />
    </li>

  <!-- pub -->
  <!-- !pub -->

  <li>
       <h2>
          <a href="https://pdf.sciencedirectassets.com/271122/1-s2.0-S0042698918X0008X/1-s2.0-S0042698918300324/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGAaCXVzLWVhc3QtMSJGMEQCIEopNFSQYv0fy91%2FbzYprnRfj6qgk%2Bh1q8Vaae0ug%2FcRAiATH57ydNq%2FwhNtJmvgN5Qy831Cp21hTWEL8CMNb52xVCqDBAi5%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAQaDDA1OTAwMzU0Njg2NSIMndjTbvQyhudx3%2FP8KtcDHo677OZ0yLaDUre7dleHz8AO2iJhKsiBLOz%2BhXr57i1wuywRgUPN4Vx1G3V2ZLir4ht8cwCKAnVQwlkGIF7%2FXmzdoBBcKoQrrN0KvlSdAXYMKRO6BEHLR7GMWMEPEkWguWy7FeCp%2F9kK3BbhQuGFqRryqVsl78Fv%2Foj3zD4jzeZof6y2WdCXKpIuEg%2BhbD9sZPjHQibxjBWBrdXqLAZHD9PiFLNe5JMD18hUWxvpRJHE80ay4UcdSBLAzADHgYlT%2F6Ej20k6LeA19lSb9IBQU462X2NOlEfHnLNkCsuFCAAVD4ZvwMcoNaqtT%2B4Q8rJ%2F0%2BhMYD9FsO2qVhnfhzYjiob%2B31ZuFIk4q%2F1qrNIg3nlhx3BwKZCM5bObTIAGqowMepM6OKafOo7v5Qi5BjXsOqiYokJHRrOyRYRkGVL4ixIMKGLhI1RHMM4obMV0mdj6XTFnfwtKBr5%2Fmsen%2FbUWUykH6XgGX%2Br%2FvZtyog%2BpznU851br%2F7vlnSNSXV2m%2BnwzzIaKOduCYr5CMHLXEGvIeAJr59rrFtIILjUAuqcvHQNioNGVmvxEeDg9vG6qsvZBBvgjyUmObLuP0%2F5G8t5sQkI%2FBK6E3YOuE8jZQ4SIA2yBcHN8EiiFMLXXm4oGOqYBAGnEMry4kxBEkoXYxB12kPAbZUWu0R0ENiKBzvWQ%2F2DTjq841TrEkm2Y6M4nZ2fk4xG0AKfzIbg3wq7FdgmPRWPKX7zDGjlyveqXjKb3qrxS9IA0BTkl0iKlNLz%2Bv%2BhwOMGJAgL4B0e8plMkq4jI0csSzJvBT%2FdHy%2FJVt%2B89D3wIdRMvJeCIMmiPTN2n2ujGmgObIDVtJIfjJ%2BKwpdBOwAx9iNlTrw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210919T084916Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYXNUH33M5%2F20210919%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=f0ffbbb56e423253472c83d5ff334acf30cf1291e171b23f01c684b48990db47&hash=cd41e80788a28f529bb38736b427555cbe1f5ef1bd478ef770b3fd7b1900ef80&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0042698918300324&tid=spdf-e0bb6212-7260-4197-93ee-a50d127a0ba0&sid=72d869e523f84447e44a5525694b9e32d515gxrqb&type=client" target="_blank">
             Categorizing natural color distributions
          </a>
       </h2>
       <div class="pic">
           <img src="images/Leaves.png" alt="9 different Objects" title="9 different Objects" width="100" height="100"/>
       </div>
       <dl>
          <dt class="authors">Authors:</dt><dd>Milojevic, Ennis, Toscani, Gegenfurtner</dd>
          <dt class="journal">Journal:</dt><dd>Vision Research</dd>
          <dt class="summary">Summary:</dt><dd class="summary">
            The natural objects that we are surrounded with virtually always contain many different shades of color, yet the visual system usually categorizes them into a single color category. We examined various image statistics and their role in categorizing the color of leaves.
           </dd>
          <dt class="citation">Citation:</dt><dd class="citation"><cite>
        Milojevic, Z., Ennis, R., Toscani, M., & Gegenfurtner, K. R. (2018). Categorizing natural color distributions. Vision research, 151, 18-30.
          </cite></dd>


       </dl>
       <br class="clear" />
    </li>

    <!-- pub -->
    <!-- !pub -->

    <li>
         <h2>
            <a href="https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2589664/Perceptually_validated_analytical_BRDFs_parameters_remapping%20(1).pdf?sequence=4" target="_blank">
               Perceptually validated analytical BRDFs parameters remapping
            </a>
         </h2>
         <div class="pic">
             <img src="images/RemappingSIGGRAPH.png" alt="9 different Objects" title="9 different Objects" width="100" height="100"/>
         </div>
         <dl>
            <dt class="authors">Authors:</dt><dd>Guarnera, Guarnera, Toscani, Glencross, Li, Hardeberg, Gegenfurtner</dd>
            <dt class="journal">Journal:</dt><dd>SIGGRAPH</dd>
            <dt class="summary">Summary:</dt><dd class="summary">
              We present an automatic BRDF remapping technique to match the appearance of a source material model to a target, providing a mapping between their parameter spaces
          </dd>
            <dt class="citation">Citation:</dt><dd class="citation"><cite>
            Guarnera, D. Y., Guarnera, G. C., Toscani, M., Glencross, M., Li, B., Hardeberg, J. Y., & Gegenfurtner, K. R. (2018). Perceptually validated analytical BRDFs parameters remapping. In ACM SIGGRAPH 2018 Talks (pp. 1-2).
            </cite></dd>

         </dl>
         <br class="clear" />
      </li>

      <!-- pub -->
      <!-- !pub -->

      <li>
           <h2>
              <a href="https://www.allpsych.uni-giessen.de/annam/paper/metzger_saliency_2018.pdf"target="_blank">
                 Haptic saliency model for rigid textured surfaces
              </a>
           </h2>
           <div class="pic">
               <img src="images/SaliencyLinear.png" alt="9 different Objects" title="9 different Objects" width="100" height="100"/>
           </div>
           <dl>
              <dt class="authors">Authors:</dt><dd>Metzger, Toscani, Valsecchi, Drewing</dd>
              <dt class="journal">Journal:</dt><dd>In International Conference on Human Haptic Sensing and Touch Enabled Computer Applications</dd>
              <dt class="summary">Summary:</dt><dd class="summary">
                When touching an object, we focus more on some of its parts ratherthan touching the whole object's surface, i.e. some parts are more salient than others. A linear model can predict touch behavior better than chance.
               </dd>
              <dt class="citation">Citation:</dt><dd class="citation"><cite>
              Metzger, A., Toscani, M., Valsecchi, M., & Drewing, K. (2018, June). Haptic saliency model for rigid textured surfaces. In International Conference on Human Haptic Sensing and Touch Enabled Computer Applications (pp. 389-400). Springer, Cham.
              </cite></dd>


           </dl>
           <br class="clear" />
        </li>


        <li>
             <h2>
                <a href="https://opg.optica.org/DirectPDFAccess/24B6D1C7-76FE-4DA0-93FDB28E3E12D3AB_383360/josaa-35-4-B256.pdf?da=1&id=383360&seq=0&mobile=no"target="_blank">
                   Hyperspectral database of fruits and vegetables
                </a>
             </h2>
             <div class="pic">
                 <img src="images/Fruits.png" alt="9 different Objects" title="9 different Objects" width="100" height="100"/>
             </div>
             <dl>
                <dt class="authors">Authors:</dt><dd>Ennis, Schiller, Toscani, Gegenfurtner</dd>
                <dt class="journal">Journal:</dt><dd>JOSA A</dd>
                <dt class="summary">Summary:</dt><dd class="summary">
                  We have built a hyperspectral database of 42 fruits and vegetables. Both the outside (skin) and inside of the objects were imaged. We used a Specim VNIR HS-CL-30-V8E-OEM mirror-scanning hyperspectral camera and took pictures at a spatial resolution of ∼57 px∕deg by 800 pixels at a wavelength resolution of ∼1.12 nm.
                 </dd>
                <dt class="citation">Citation:</dt><dd class="citation"><cite>
                Ennis, R., Schiller, F., Toscani, M., & Gegenfurtner, K. R. (2018). Hyperspectral database of fruits and vegetables. JOSA A, 35(4), B256-B266.
                </cite></dd>


             </dl>
             <br class="clear" />
          </li>
                </ul>

<!-- pub -->


               <a id="year17" />
                  <h1>
                     2017
                  </h1>
                  <ul>
<!-- !pub -->
 					         <li>
                        <h2>
                           <a href="https://jov.arvojournals.org/article.aspx?articleid=2652665" target="_blank">
                              Foveal to peripheral extrapolation of brightness within objects
                           </a>
                        </h2>
                        <div class="pic">
                          <img src="images/ANIMATION_FILLOUT.gif" alt="9 different Objects" title="9 different Objects" width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Toscani, Gegenfurtner, Valsecchi</dd>
                           <dt class="journal">Journal:</dt><dd>Journal of Vision</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                              Results indicate that our visual system uses the brightness of the foveally viewed surface area to estimate the brightness of areas in the periphery. However, this mechanism is selectively applied within an object's boundary.
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                              Toscani, M., Gegenfurtner, K. R., & Valsecchi, M. (2017). Foveal to peripheral extrapolation of brightness within objects. Journal of vision, 17(9), 14-14.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->
<!-- !pub -->
 					          <li>
                        <h2>
                           <a href="https://www.sciencedirect.com/science/article/pii/S0960982217305444" target="_blank">
                              Seeing lightness in the dark
                           </a>
                        </h2>
                        <div class="pic">
                          <img src="images/Scotopic.png" alt="materials" title="materials" width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Ennis, Toscani, Gegenfurtner</dd>
                           <dt class="journal">Journal:</dt><dd>Current Biology</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                              Here we show that the perception of lightness is  different for night vision: surfaces that appear to be white under daylight conditions, at best, appear medium gray under night vision, suggesting that activation of the cones is necessary for the perception of white.
                            </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                              Ennis, R., Toscani, M., & Gegenfurtner, K. R. (2017). Seeing lightness in the dark. Current Biology, 27(12), R586-R588.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->
<!-- !pub -->
					           <li>
                        <h2>
                           <a href="https://www.sciencedirect.com/science/article/pii/S0042698916301961" target="_blank">
                              Lightness perception for matte and glossy complex shapes
                           </a>
                        </h2>
                        <div class="pic">
                          <img src="images/lightnessgloss.jpg" width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Toscani, Valsecchi, Gegenfurtner</dd>
                           <dt class="journal">Journal:</dt><dd>Vision Research</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                              We show that human observers effectively ignore specular reflections while evaluating the lightness of glossy objects, which results in a bias to perceive glossy objects as darker.
                            </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                              Toscani, M., Valsecchi, M., & Gegenfurtner, K. R. (2017). Lightness perception for matte and glossy complex shapes. Vision research, 131, 82-95.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>

<!-- pub -->

<!-- !pub -->
					           <li>
                        <h2>
                           <a href="https://jov.arvojournals.org/article.aspx?articleid=2598922" target="_blank">
                              Differences in illumination estimation in #thedress
                           </a>
                        </h2>
                        <div class="pic">
                          <img src="images/DressProbe.gif"  width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Toscani, Gegenfurtner, Doerschner</dd>
                           <dt class="journal">Journal:</dt><dd>Journal of Vision</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                            We investigated whether people report different colors for #thedress because they have different assumptions about the illumination in the scene. We introduced a spherical illumination probe into the original photograph, and let observers adjust the probe such that it would appear as a white sphere in the scene.
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                              Toscani, M., Gegenfurtner, K. R., & Doerschner, K. (2017). Differences in illumination estimation in# thedress. Journal of Vision, 17(1), 22-22.
                           </cite></dd>


                        </dl>
                        <br class="clear" />
                     </li>

<!-- pub -->


                  <a id="year16" />
                  <h1>
                     2016
                  </h1>
                  <ul>
<!-- !pub -->
                   <li>
                        <h2>
                           <a href="https://jov.arvojournals.org/article.aspx?articleid=2594669" target="_blank">
                              Lightness perception for surfaces moving through different illumination levels
                           </a>
                        </h2>
                        <div class="pic">
                          <img src="images/ANIMATION_PENDULUM.gif" alt="Stimuli" title="Stumuli" width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Toscani, Zdravković, Gegenfurtner</dd>
                           <dt class="journal">Journal:</dt><dd>Journal of Vision</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                              We asked participants for lightness matches of a virtual three-dimensional target moving through a light field. We found that the target appeared differently depending on the direction of motion in the light field. This indicates that humans fail to interate all the available information for lightness perception.
                            </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                            Toscani, M., Zdravković, S., & Gegenfurtner, K. R. (2016). Lightness perception for surfaces moving through different illumination levels. Journal of vision, 16(15), 21-21.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>

<!-- pub -->
<!-- !pub -->

                  <a id="year15" />
                  <h1>
                     2015
                  </h1>

                  <ul>

<!-- !pub -->

                     <li>
                        <h2>
                           <a href="https://www.sciencedirect.com/science/article/pii/S0042698915001595" target="_blank">
                              Statistical correlates of perceived gloss in natural images
                           </a>
                        </h2>
                        <div class="pic">
                           <img src="images/GlossClassification.png"  width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Wiebel, Toscani, Gegenfurtner</dd>
                           <dt class="journal">Journal:</dt><dd>Vision Research</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                             We analyzed many images of natural surfaces
to search for potential statistical correlates of perceived gloss. We found that skewness correlates
with gloss when using rendered stimuli, but that the standard deviation, a measure of contrast, correlates
better with perceived gloss when using photographs of natural surfaces.
                           </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                            Wiebel, C. B., Toscani, M., & Gegenfurtner, K. R. (2015). Statistical correlates of perceived gloss in natural images. Vision research, 115, 175-187.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->

<!-- !pub -->
                     <li>
                        <h2>
                           <a href="https://reader.elsevier.com/reader/sd/pii/S0960982215004947?token=78E90370048A2D7BC4ABBAD86A1633C00F180B033F967E6DA95E5467E1B4618566DD15E289E514EE5F5F5B5B236A850A&originRegion=eu-west-1&originCreation=20210919100532" target="_blank">
                              The many colours of ‘the dress’
                           </a>
                        </h2>
                        <div class="pic">
                           <img src="images/Dress2015.png" alt="a green liquid" title="a green liquid" width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Gegenfurtner, Bloj, Toscani</dd>
                           <dt class="journal">Journal:</dt><dd>Current Biology</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                            The popular image of this dress has shown impressively that our perception of the world is not just a result of physical properties recorded by our senses. Rather, we make assumptions about the world that guide the interpretation of sensory data, and these assumptions can be quite different for different individuals.
                            </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                              Gegenfurtner, K. R., Bloj, M., & Toscani, M. (2015). The many colours of ‘the dress’. Current Biology, 25(13), R543-R544.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->

<!-- !pub -->
                     <li>
                        <h2>
                           <a href="xxx" target="_blank">
                              Effect of fixation positions on perception of lightness
                           </a>
                        </h2>
                        <div class="pic">
                           <img src="images/LightnessMatching.png" alt="liquid motion vectors" title="liquid motion vectors" width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Toscani, Valsecchi, Gegenfurtner</dd>
                           <dt class="journal">Journal:</dt><dd>Human Vision and Electronic Imaging XX (Vol. 9394, p. 93940R). International Society for Optics and Photonics.</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                             We review two studies showing that eye fixations are partially responsible for the selection of information from a scene that allows
 the visual system to estimate the reflectance of a surface.
                          </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                              Toscani, M., Valsecchi, M., & Gegenfurtner, K. R. (2015, March). Effect of fixation positions on perception of lightness. In Human Vision and Electronic Imaging XX (Vol. 9394, p. 93940R). International Society for Optics and Photonics.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->

<!-- !pub -->

<!-- pub -->
                  </ul>



                  <a id="year13" />
                  <h1>
                     2013
                  </h1>
                  <ul>
<!-- !pub -->
<li>
   <h2>
      <a href="https://iovs.arvojournals.org/article.aspx?articleid=2193856" target="_blank">
         Perceived numerosity is reduced in peripheral vision
      </a>
   </h2>
   <div class="pic">
      <img src="images/Numerosity.gif" alt="a cup toppeling over" title="a cup toppeling over" width="100" height="100"/>
   </div>
   <dl>
      <dt class="authors">Authors:</dt><dd>Valsecchi, Toscani, Gegenfurtner</dd>
      <dt class="journal">Journal:</dt><dd>Journal of Vision</dd>
      <dt class="summary">Summary:</dt><dd class="summary">
         We found that the perceived numerosity of a peripheral cloud of dots was judged to be inferior to the one of a central cloud of dots, particularly when the dots were highly clustered.
       </dd>
      <dt class="citation">Citation:</dt><dd class="citation"><cite>
         Valsecchi, M., Toscani, M., & Gegenfurtner, K. R. (2013). Perceived numerosity is reduced in peripheral vision. Journal of vision, 13(13), 7-7.
      </cite></dd>

   </dl>
   <br class="clear" />
</li>
<!-- pub -->

<!-- !pub -->
                     <li>
                        <h2>
                           <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2013.0056?keytype=ref&ijkey=crGz21KGWqcz6oW" target="_blank">
                              Selection of visual information for lightness judgements by eye movements
                           </a>
                        </h2>
                        <div class="pic">
                           <img src="images/Moons.gif" alt="a tee pot rotating" title="a tee pot rotating" width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Toscani, Valsecchi, Gegenfurtner</dd>
                           <dt class="journal">Journal:</dt><dd>PHILOS T R SOC B</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                              When judging the lightness of objects, the visual system needs to estimate global reflectance based on a number of local samples that differ in luminance. Here, we show that eye fixations play a prominent role in this selection process.
                            </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                              Toscani, M., Valsecchi, M., & Gegenfurtner, K. R. (2013). Selection of visual information for lightness judgements by eye movements. Philosophical Transactions of the Royal Society B: Biological Sciences, 368(1628), 20130056.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->

<!-- !pub -->
                     <li>
                        <h2>
                           <a href="https://www.pnas.org/content/pnas/110/27/11163.full.pdf" target="_blank">
                              Optimal sampling of visual information for lightness judgments
                           </a>
                        </h2>
                        <div class="pic">
                           <img src="images/coneFixations.gif" width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Toscani, Valsecchi, Gegenfurtner</dd>
                           <dt class="journal">Journal:</dt><dd>PNAS</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                              Observers based their judgments on the brightest parts of the objects, which they tend to fixate. With a gaze-contingent display setup we showed a causal link between the luminance at fixation and  lightness appearance. Simulations show that  objects brightest regions are particularly informative about surface reflectance.
                            </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                            Toscani, M., Valsecchi, M., & Gegenfurtner, K. R. (2013). Optimal sampling of visual information for lightness judgments. Proceedings of the National Academy of Sciences, 110(27), 11163-11168.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->

                  <a id="year12" />
                  <h1>
                     2012
                  </h1>
                  <ul>
<!-- !pub -->
<!-- pub -->

<!-- !pub -->
                     <li>
                        <h2>
                           <a href="https://d1wqtxts1xzle7.cloudfront.net/47218662/Role_of_eye_movements_in_chromatic_induction.pdf?1468424127=&response-content-disposition=inline%3B+filename%3DRole_of_eye_movements_in_chromatic_induc.pdf&Expires=1632076675&Signature=NtLvzieZM-~~WKee6mur7waY3-r-FexwDY3dj-OT7VyXd2Kxi48-ZBmw3PMbUH6UMoevs9-bGTq5HItmfrUiltmIboZ69QTFHRJIxQ0imWsAOq8ErSSCU7cQTdW4vV49P~MEYuV4l7W~QZphxpGNqJXq0tA1abMPfwM9YrxKd6Pr9wVLVC-tYLhkKkBG0VK8BtSJRCjvD72Md2nzrHf4h7gwPyKjZHnjrojMq1lo9PCMYJTi4bV41Xdcpgzj6ubOJBFP8i3FpqkIqQIKke4KsE0OYujfD87sFtaeGwxzirqAOVJ0RxjzZeh5LGgOar64gwYiOqF5bCXYk9FGXslGsw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA" target="_blank">
                              Role of eye movements in chromatic induction
                           </a>
                        </h2>
                        <div class="pic">
                         <img src="images/eyeinduction.jpg"  width="110" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Granzier, Toscani, Gegenfurtner</dd>
                           <dt class="journal">Journal:</dt><dd>JOSA A</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                             Do eye movements explain individual differences in chromatic induction?. We found that they only play a role under artificial (forced looking) viewing conditions and that eye movements do not
seem to play a large role for chromatic induction under natural viewing conditions. 
                            </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                              Granzier, J. J., Toscani, M., & Gegenfurtner, K. R. (2012). Role of eye movements in chromatic induction. JOSA A, 29(2), A353-A365.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->

<!-- !pub -->
                     <li>
                        <h2>
                           <a href="" target="_blank">
                              Fearful expressions enhance recognition memory: electrophysiological evidence
                           </a>
                        </h2>
                        <div class="pic">
                           <img src="images/FacesERP.png"  width="100" height="100"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Righi, Marzi, Toscani, Baldassi, Ottonello, Viggiano</dd>
                           <dt class="journal">Journal:</dt><dd>Acta psychologica</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                            Our results suggest that face recognition is modulated by top-down influences from brain areas associated with emotional memory, enhancing encoding and retrieval in particular for fearful emotional expressions.
                            </dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                            Righi, S., Marzi, T., Toscani, M., Baldassi, S., Ottonello, S., & Viggiano, M. P. (2012). Fearful expressions enhance recognition memory: electrophysiological evidence. Acta psychologica, 139(1), 7-18.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->

                  </ul>

                  <a id="year10" />
                  <h1>
                     2010
                  </h1>
                  <ul>
<!-- !pub -->
                     <li>
                        <h2>
                           <a href="" target="_blank">
                            Alpha waves: a neural signature of visual suppression
                           </a>
                        </h2>
                        <div class="pic">
                          <img src="images/Alpha.jpg" alt="shape from smear" title="shape from smear" width="100" height="110"/>
                        </div>
                        <dl>
                           <dt class="authors">Authors:</dt><dd>Toscani, Marzi, Righi,Viggiano, Baldassi</dd>
                           <dt class="journal">Journal:</dt><dd>Experimental brain research</dd>
                           <dt class="summary">Summary:</dt><dd class="summary">
                             We measured visual suppression occurring during short closures of the eyelids while stimulating the
retinae through the palate, independent on the eyelids. With eyes steady closed, detection thresholds were hihger and alpha activity increased. We interpreted this as a central effect, probably due to a "gathing functin".
</dd>
                           <dt class="citation">Citation:</dt><dd class="citation"><cite>
                          Toscani, M., Marzi, T., Righi, S., Viggiano, M. P., & Baldassi, S. (2010). Alpha waves: a neural signature of visual suppression. Experimental brain research, 207(3), 213-219.
                           </cite></dd>

                        </dl>
                        <br class="clear" />
                     </li>
<!-- pub -->

<!-- pub -->
                  </ul>


<!-- !footer -->

<!-- footer -->

            </div>
<!-- content -->

         </div>
<!-- inner_wrapper -->
      </div>
<!-- outer_wrapper -->
   </body>
</html>
